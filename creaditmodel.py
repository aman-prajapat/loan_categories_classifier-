# -*- coding: utf-8 -*-
"""CreaditModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L3eMnPUIPKI3m1WZ6iZZa5GNyFFX6bZU

## Problem statment
predict whether or not give loan to customer

# Banking

# Asset = Loan product
Housing loan
Personal loan
Vehicle loan
Group loan
Education loan
Credit Card

# Liability
Current account
Savings account
(Casa)

Fixed deposit
Recurring Deposit
(Term Deposits)

With respect to Bank

# NPA
NPA = Non Performing Asset (ex vijay maliya)
NPA = Loan that is defaulted


1. Disbursed Amount = Loan amount given to a customer

2. OSP = Out Standing Principle ()
1 Lakh loan
8000 EMI
OSP
40000 pay
60000 bakaya = Balance = OSP
OSP should be zero at the end of loan cycle


3. DPD = Days past due
DPD = 2
DPD ideally zero

"Defaulted" DPD  > 0


4. PAR
Portfolio at risk
OSP when DPD > 0



5. NPA
Loan account when DPD > 90 days
NPA account

NPA types-

GNPA = Gross NPA (3-5 %) = OSP dafault
NNPA = Net NPA = (0.01 to 0.06 %) = Provisioning Amount subtracted

Bank quality assess, GNPA value

# Credit Risk Types in Banking

DPD (Zero) : NDA (Non delinquint account) = No default account = Timely payment EMI
DPD (0 to 30) : SMA1 (Standard Monitoring Account)
DPD (31 to 60) : SMA2 (Standard Monitoring Account)
DPD (61 to 90) : SMA3 (Standard Monitoring Account)
DPD (90 to 180) : NPA

DPD (>180) : Writen-off (Loan which is not present, remove from loan DB)

written-off to NPA improve = Loan Portfolio quality of the bank will be better = Market sentiment will be good = Stock price will improve


TL = Trade line or lones account
"""

# !pip install statsmodels sklearn scipy os warnings

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
from scipy.stats import chi2_contingency
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support
import warnings
import os

from google.colab import drive
drive.mount('/content/drive/')
# /content/drive/MyDrive/dataset



# Load the dataset
a1 = pd.read_excel("/content/drive/MyDrive/case_study1.xlsx")
a2 = pd.read_excel("/content/drive/MyDrive/case_study2.xlsx")

df1 = a1.copy()
df2 = a2.copy()

df1.info()

df1[df1 == -99999].count()

# Remove nulls
df1 = df1.loc[df1['Age_Oldest_TL'] != -99999]
df1 = df1.loc[df1['Age_Newest_TL'] != -99999]
# df1.shape

df2[df2 == -99999].count()

columns_to_be_removed = []
threshold = 10000
for i in df2.columns:
    if df2.loc[df2[i] == -99999].shape[0] > threshold:
        columns_to_be_removed .append(i)

df2 = df2.drop(columns_to_be_removed, axis =1)

df2.shape

for i in df2.columns:
    df2 = df2.loc[ df2[i] != -99999 ]

# Checking common column names
for i in list(df1.columns):
    if i in list(df2.columns):
        print (i)

# Merge the two dataframes, inner join so that no nulls are present
df = pd.merge ( df1, df2, how ='inner', left_on = ['PROSPECTID'], right_on = ['PROSPECTID'] )

# check how many columns are categorical
for i in df.columns:
    if df[i].dtype == 'object':
        print(i)

for i in ['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']:
    print(pd.crosstab(df[i], df['Approved_Flag']),"\n")

# Chi-square test
# measure 1 on chi2 value if chi2 val is greater than tab value than have significant diff
# measure 2 on pval if pval is lessser than alpha than have significant diff
for i in ['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']:
    chi2, pval, _, _ = chi2_contingency(pd.crosstab(df[i], df['Approved_Flag']))
    print(i, '---', f"{pval}" , "---",chi2)

# Since all the categorical features have pval <=0.05, we will accept all,Ho rejected

# VIF for numerical columns
numeric_columns = []
for i in df.columns:
    if df[i].dtype != 'object' and i not in ['PROSPECTID','Approved_Flag']:
        numeric_columns.append(i)

# VIF sequentially check

vif_data = df[numeric_columns]
total_columns = vif_data.shape[1]
columns_to_be_kept = []
column_index = 0

total_columns

removedCol = []
for i in range (0,total_columns):
    vif_value = variance_inflation_factor(vif_data, column_index)
    print (column_index,'---',vif_value,"    ",i)

    if vif_value <= 6:
        columns_to_be_kept.append( numeric_columns[i] )

        column_index = column_index+1

    else:
        removedCol.append((numeric_columns[i],vif_value))
        vif_data = vif_data.drop([numeric_columns[i] ] , axis=1)

removedCol  #removed col name and there VIF

# check Anova for columns_to_be_kept

from scipy.stats import f_oneway

columns_to_be_kept_numerical = []

for i in columns_to_be_kept:
    a = list(df[i])
    b = list(df['Approved_Flag'])

    group_P1 = [value for value, group in zip(a, b) if group == 'P1']
    group_P2 = [value for value, group in zip(a, b) if group == 'P2']
    group_P3 = [value for value, group in zip(a, b) if group == 'P3']
    group_P4 = [value for value, group in zip(a, b) if group == 'P4']


    f_statistic, p_value = f_oneway(group_P1, group_P2, group_P3, group_P4)

    if p_value <= 0.05:
        columns_to_be_kept_numerical.append(i)

"""# feature selection is done for cat and num features

"""

# listing all the final features
features = columns_to_be_kept_numerical + ['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']
df = df[features + ['Approved_Flag']]

list(df.columns)

"""## **# Label encoding for the categorical features**"""

# ['MARITALSTATUS', 'EDUCATION', 'GENDER' , 'last_prod_enq2' ,'first_prod_enq2']

df[['MARITALSTATUS', 'EDUCATION', 'GENDER' , 'last_prod_enq2' ,'first_prod_enq2']].nunique()

# Ordinal feature -- EDUCATION
# SSC            : 1
# 12TH           : 2
# GRADUATE       : 3
# UNDER GRADUATE : 3
# POST-GRADUATE  : 4
# OTHERS         : 1
# PROFESSIONAL   : 3

df.loc[df['EDUCATION'] == 'SSC',['EDUCATION']]              = 1
df.loc[df['EDUCATION'] == '12TH',['EDUCATION']]             = 2
df.loc[df['EDUCATION'] == 'GRADUATE',['EDUCATION']]         = 3
df.loc[df['EDUCATION'] == 'UNDER GRADUATE',['EDUCATION']]   = 3
df.loc[df['EDUCATION'] == 'POST-GRADUATE',['EDUCATION']]    = 4
df.loc[df['EDUCATION'] == 'OTHERS',['EDUCATION']]           = 1
df.loc[df['EDUCATION'] == 'PROFESSIONAL',['EDUCATION']]     = 3

# prompt: from the above cell create a dic which have key as value after ==  and value is assignment value

education_mapping = {
    'SSC': 1,
    '12TH': 2,
    'GRADUATE': 3,
    'UNDER GRADUATE': 3,
    'POST-GRADUATE': 4,
    'OTHERS': 1,
    'PROFESSIONAL': 3
}

# prompt: export education_mapping as json file

import json

with open('/content/education_mapping.json', 'w') as f:
  json.dump(education_mapping, f)

df['EDUCATION'].value_counts()
df['EDUCATION'] = df['EDUCATION'].astype(int)
df.info()

labels = dict()
for i in ['MARITALSTATUS','GENDER', 'last_prod_enq2' ,'first_prod_enq2']:
  labels[i] = list(df[i].unique()
)

# list(df['MARITALSTATUS'].unique())
labels

# prompt: write a code to save labels dictionary in json file

import json

with open('/content/drive/My Drive/CatColumnsAndlabels.json', 'w') as f:
  json.dump(labels, f)

import json

# Open the JSON file
with open('/content/drive/My Drive/CatColumnsAndlabels.json', 'r') as f:
  data = json.load(f)

# Store the values in a dictionary
labels = data
labels

# getting OHE data for Nominal data
df_encoded = pd.get_dummies(df, columns=['MARITALSTATUS','GENDER', 'last_prod_enq2' ,'first_prod_enq2'])
df_encoded.head()

df_encoded.columns = df_encoded.columns.str.lstrip('_')
df_encoded.columns

"""# **Machine Learing model fitting**"""

y = df_encoded['Approved_Flag']
x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

import pickle
finalColumnsName = x.columns
with open('/content/drive/MyDrive/finalColumnsName.pkl', 'wb') as f:
    pickle.dump(finalColumnsName, f)

with open('/content/drive/MyDrive/finalColumnsName.pkl', 'rb') as f:
    finalColumnsName = pickle.load(f)
finalColumnsName

finalColumnsName.shape

# 1. Random Forest


rf_classifier = RandomForestClassifier(n_estimators = 200, random_state=42)
rf_classifier.fit(x_train, y_train)

y_pred = rf_classifier.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)

print ()
print(f'Accuracy: {accuracy}')
print ()

precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)

for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v}:")
    print(f"Precision: {precision[i]}")
    print(f"Recall: {recall[i]}")
    print(f"F1 Score: {f1_score[i]}")
    print()

# 2. xgboost

import xgboost as xgb
from sklearn.preprocessing import LabelEncoder

xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',  num_class=4)



y = df_encoded['Approved_Flag']
x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )


label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)


x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)



xgb_classifier.fit(x_train, y_train)
y_pred = xgb_classifier.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print ()
print(f'Accuracy: {accuracy:.2f}')
print ()

precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)

for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v}:")
    print(f"Precision: {precision[i]}")
    print(f"Recall: {recall[i]}")
    print(f"F1 Score: {f1_score[i]}")
    print()

# 3. Decision Tree
from sklearn.tree import DecisionTreeClassifier


y = df_encoded['Approved_Flag']
x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)


dt_model = DecisionTreeClassifier(max_depth=20, min_samples_split=10)
dt_model.fit(x_train, y_train)
y_pred = dt_model.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print ()
print(f"Accuracy: {accuracy:.2f}")
print ()

precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)

for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v}:")
    print(f"Precision: {precision[i]}")
    print(f"Recall: {recall[i]}")
    print(f"F1 Score: {f1_score[i]}")
    print()

"""
### xgboost is giving me best results
### We will further finetune it"""

# Hyperparameter tuning in xgboost
from sklearn.model_selection import GridSearchCV
x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)

# Define the XGBClassifier with the initial set of hyperparameters
xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=4)


# Define the parameter grid for hyperparameter tuning

param_grid = {
    'n_estimators': [100,200],
    'max_depth': [4,6,8],
    'learning_rate': [0.01,0.1,0.2],
    'alpha': [0.1,0.5,1,10]
}

grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid_search.fit(x_train, y_train)

# Print the best hyperparameters
print("Best Hyperparameters:", grid_search.best_params_)

# Evaluate the model with the best hyperparameters on the test set
best_model = grid_search.best_estimator_
accuracy = best_model.score(x_test, y_test)
print("Test Accuracy:", accuracy)

# Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}

# prompt: from above cell gridsearch cv print all parameter and result also add accuracy and loss

# Print the results of the grid search
print("GridSearchCV Results:")
print()
print("Parameters:", grid_search.cv_results_['params'])
print("Mean Test Accuracy:", grid_search.cv_results_['mean_test_score'])
print("Standard Deviation:", grid_search.cv_results_['std_test_score'])
print()

# Print the best parameters and score
print("Best Parameters:", grid_search.best_params_)
print("Best Test Accuracy:", grid_search.best_score_)

# Evaluate the best model on the test set
y_pred = best_model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
loss = log_loss(y_test, best_model.predict_proba(x_test))
print("Test Accuracy:", accuracy)
print("Test Loss:", loss)

# Getting best results with these-
# colsample_bytree  0.7
# learning_rate     0.2
# max_depth         3
# alpha             10.0
# n_estimators      200

# Retrain on the new parameters
model = xgb.XGBClassifier(objective='multi:softmax',
                         colsample_bytree = 0.7,
                         learning_rate    = 0.3,
                         max_depth        = 5,
                         alpha            = 0.3,
                         n_estimators     = 200,
                         eval_metric = 'merror',
                         num_class = 4)


model.fit(x_train, y_train)

from sklearn.metrics import mean_squared_error

# Predicting on the testing set
y_pred = model.predict(x_test)

# Calculate loss metrics
print()
r2 = r2_score(y_test, y_pred)
print('R-squared:', r2)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print('RMSE:', rmse)
stddev = y_test.std()
print('Stddev difference', rmse/stddev)

model.score(x_test,y_test)

# save the model
import pickle
filename = 'eps_v1.pkl'
pickle.dump(model, open(filename,'wb'))


# load_model = pickle.load(open(filename,'rb'))


# arg = x_train[:2]
# load_model.predict(arg)


# ROCE (%)
# 1.91


# CASA (%)
# 39.47


# Return on Equity / Networth (%)
# 14.36


# Non-Interest Income/Total Assets (%)
# 0.68


# Operating Profit/Total Assets (%)
# 0.27


# Operating Expenses/Total Assets (%)
# 1.68

# Interest Expenses/Total Assets (%)
# 3.3


# Face_value
# 2


# Basic EPS (Rs.)
# 27.28

model.predict(x_test)

